{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLEX TME 10 Feature Selection Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the TME is to learn various techniques of feature selection.\n",
    "Data (both data sets are provided)\n",
    "• Molecular classification of leukemia data set of Golub et al. 1999 contains gene expressions of 72 patients and 3562 genes.\n",
    "• Breast cancer data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will need to load at least the following packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Repeat the same analyses for the two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 3562)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read the data:\n",
    "# For the Golub et al. 1999 data\n",
    "X = pd.read_csv('Golub_X',sep=\" \") # Observations\n",
    "y = pd.read_csv('Golub_y',sep=\" \") # Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the Breast cancer data\n",
    "X = pd.read_csv(\"Breast.txt\",sep=\" \")\n",
    "y = X.values[:,30] # Classes\n",
    "X = X.values[:,0:29] # Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sklearn Python library only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  A simple heuristic approach \n",
    "\n",
    "It is to delete features whose variance is less then a threshold. Try it (with two different arbitrary thresholds) but do not expect this method to return an optimal\n",
    "performance (although it can be quite efficient on some data sets).\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.82821197, -0.35332152,  1.68447255, ..., -0.14661996,\n",
       "         1.08612862, -0.24367526],\n",
       "       [ 1.5784992 ,  0.45578591,  1.56512598, ...,  0.85422232,\n",
       "         1.95328166,  1.15124203],\n",
       "       [-0.76823332,  0.25350905, -0.59216612, ...,  1.98783917,\n",
       "         2.17387323,  6.04072615],\n",
       "       ...,\n",
       "       [ 0.70166686,  2.04377549,  0.67208442, ...,  0.32647934,\n",
       "         0.41370467, -1.10357792],\n",
       "       [ 1.83672491,  2.33440316,  1.98078127, ...,  3.1947936 ,\n",
       "         2.28797231,  1.9173959 ],\n",
       "       [-1.80681144,  1.22071793, -1.81279344, ..., -1.30468267,\n",
       "        -1.7435287 , -0.04809589]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Univariate feature selection with statistical tests\n",
    "In order to get rid of features which are not statistically significant with respect to the vector of class. Try the SelectFdr\n",
    "function that computes p-values for an estimated false discovery rate.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chi2 onky takes positive values so we cannot use above defined breast cancer data with this stats function. \n",
    "#Let's try ANOVA F-value\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFdr, f_classif\n",
    "X_new = SelectFdr(f_classif, alpha=0.01).fit_transform(X, y)\n",
    "X_new.shape\n",
    "\n",
    "# we remove 4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load and return the breast cancer wisconsin dataset (classification).\n",
    "\n",
    "    The breast cancer dataset is a classic and very easy binary classification\n",
    "    dataset.\n",
    "\n",
    "    =================   ==============\n",
    "    Classes                          2\n",
    "    Samples per class    212(M),357(B)\n",
    "    Samples total                  569\n",
    "    Dimensionality                  30\n",
    "    Features            real, positive\n",
    "    =================   ==============\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    return_X_y : boolean, default=False\n",
    "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
    "        See below for more information about the `data` and `target` object.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Bunch\n",
    "        Dictionary-like object, the interesting attributes are:\n",
    "        'data', the data to learn, 'target', the classification labels,\n",
    "        'target_names', the meaning of the labels, 'feature_names', the\n",
    "        meaning of the features, and 'DESCR', the\n",
    "        full description of the dataset.\n",
    "\n",
    "    (data, target) : tuple if ``return_X_y`` is True\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "    The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
    "    downloaded from:\n",
    "    https://goo.gl/U2Uwz2\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Let's say you are interested in the samples 10, 50, and 85, and want to\n",
    "    know their class name.\n",
    "\n",
    "    >>> from sklearn.datasets import load_breast_cancer\n",
    "    >>> data = load_breast_cancer()\n",
    "    >>> data.target[[10, 50, 85]]\n",
    "    array([0, 1, 0])\n",
    "    >>> list(data.target_names)\n",
    "    ['malignant', 'benign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectFdr, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape\n",
    "\n",
    "X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.L1-based feature selection\n",
    "it is designed to find an optimal solution.  The sparsity parameter is important (since it controls the number of non-zero parameters:  it too many parameters\n",
    "are kept, no really feature selection; if too few parameters are chosen, it is possible that the accuracy is very poor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)  Logistic regression penalized by the L1 penalty term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.Lasso(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)  A support vector machine penalized by the L1 penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC(C=C, penalty=\"l1\", dual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)  Explore the Elastic Net which is a compromise between the L1 and L2 penalty terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet(alpha=alpha, l1_ratio=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Features to keep\n",
    "How many features do you keep using these different methods?  It is quite normal that each method selects a different number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Method leading to the best perf\n",
    "What method leads to the best performance (on the given data sets) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "• The original Lasso paper: Tibshirani,  R. (1996).  Regression shrinkage and selection via the lasso.  J. Royal.  Statist.\n",
    "Soc B., Vol.  58, No.  1, pages 267–288http://statweb.stanford.edu/~tibs/lasso/lasso.pdf\n",
    "\n",
    "•T. Hastie, R. Tibshirani, and M. Wainwright.Statistical Learning with Sparsity. The Lassoand Generalizations.(a good book)\n",
    "https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
